{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston housing prices - A project report on model evaluation and validation\n",
    "\n",
    "#### 1) Statistical Analysis and Data Exploration\n",
    "\n",
    "##### Number of data points (houses)?  \n",
    "    506\n",
    "    \n",
    "##### Number of features?  \n",
    "    13\n",
    "    \n",
    "##### Minimum and maximum housing prices?  \n",
    "    Minimum price: 5 Maximum price: 50\n",
    "    \n",
    "##### Mean and median Boston housing prices?  \n",
    "    Mean: 22.533 Median: 21.2\n",
    "    \n",
    "##### Standard deviation?  \n",
    "    9.188\n",
    "    \n",
    "#### 2) Evaluating Model Performance\n",
    "\n",
    "##### Which measure of model performance is best to use for predicting Boston housing data and analyzing the errors?\n",
    "    \"explained_variance_score\" is used as the measure of model\n",
    "    performance.\n",
    "    \n",
    "##### Why do you think this measurement most appropriate? Why might the other measurements not be appropriate here?\n",
    "    Using other metrics, lead GridSearch to choose models that were less complex\n",
    "    and underfit the data. I got really bizarre results when I tried to correlate \n",
    "    the predicted prices with the actual prices. I evaluated mean_absolute_error,\n",
    "    mean_squared_error, r2_score and finally explained_variance_score. I still \n",
    "    intuitively couldn't understand explained_variance_score performs well though.\n",
    "    \n",
    "##### Why is it important to split the Boston housing data into training and testing data? What happens if you do not do this?\n",
    "    If we don't split the data into test and training set, and train the\n",
    "    model on complete data set we may end up with the model that\n",
    "    overfits just the training data and fails to generalize. So it\n",
    "    is important to split the data into training and testing sets, and\n",
    "    use the testing set for validation of the model.\n",
    "    \n",
    "##### What does grid search do and why might you want to use it?\n",
    "    \"grid search\" takes a parameter grid as one of the inputs (parameter grid\n",
    "    depends on the estimator used) and runs training for each of the\n",
    "    parameter from the grid. It also takes scorer function as input\n",
    "    and uses it to compare the runs and choose the best parameter\n",
    "    set. It is useful when we don't know what parameter will be ideal\n",
    "    for an estimator and want to try a set of parameters and choose\n",
    "    the best one.\n",
    "    \n",
    "##### Why is cross validation useful and why might we use it with grid search?\n",
    "    Splitting data into training and test set makes less data\n",
    "    available for training. With cross validation in place, we get the\n",
    "    advantage of training and testing on the complete data set. This is \n",
    "    because, in each iteration of cross validation, portion of data is used\n",
    "    for testing and the remaining for training. This repeats till the model\n",
    "    is trained and tested on all the available data.\n",
    "\n",
    "#### 3) Analyzing Model Performance\n",
    "\n",
    "##### Look at all learning curve graphs provided. What is the general trend of training and testing error as training size increases?\n",
    "    The training error increases as the training size increased. This\n",
    "    is because, when the data size is smal, any model can easily fit\n",
    "    it reasonably well. But as the training size increases, it is not\n",
    "    so easy to find a model that fits all the data points well. So the\n",
    "    training error increases with the size of the data.\n",
    "\n",
    "    On the other hand, the testing error is high when the data size is small. This\n",
    "    is because when the size of the data is less, any trivial model\n",
    "    can fit the data well but the model won't generalize for the new\n",
    "    data points. But as the training size increases, the model gets\n",
    "    complex in an attempt to fit the data reasonably well, and it is\n",
    "    likely that it fits the testing data also well. So, the training\n",
    "    error reduces with the increase in the size of the data.\n",
    "    \n",
    "##### Look at the learning curves for the decision tree regressor with max depth 1 and 10 (first and last learning curve graphs). When the model is fully trained does it suffer from either high bias/underfitting or high variance/overfitting?\n",
    "    With max_depth 1, when the model is fully trained, it suffers from high\n",
    "    bias(underfitting) as the model is not complex enough for the\n",
    "    data. Throwing more data into the model doesn't seem to help\n",
    "    because the model couldn't get complex enough to represent the\n",
    "    data accurately.\n",
    "\n",
    "    With max_depth 10, when the model is fully\n",
    "    trained, it suffers from high variance i.e. it overfits the\n",
    "    data. This was shown in the learning curve while observing the\n",
    "    changes to training error. With high enough max_depth, the model\n",
    "    got complex, closely followed the data (we can say it pretty much\n",
    "    memorized the data) but failed to generalize for new set of data.\n",
    "    \n",
    "##### Look at the model complexity graph. How do the training and test error relate to increasing model complexity? Based on this relationship, which model (max depth) best generalizes the dataset and why?\n",
    "    The training and testing error reduces as the model\n",
    "    complexity increases. But after certain stage, the model becomes\n",
    "    unnecessarily complex as there is no improvement in the model\n",
    "    performance but just the complexity increases.\n",
    "\n",
    "#### 4) Model Prediction\n",
    "\n",
    "##### Model makes predicted housing price with detailed model parameters (max depth) reported using grid search. Note due to the small randomization of the code it is recommended to run the program several times to identify the most common/reasonable price/model complexity.\n",
    "    \n",
    "    Desired model complexity: 5\n",
    "    Predicted price: 20.96776316\n",
    "    \n",
    "##### Compare prediction to earlier statistics and make a case if you think it is a valid model.\n",
    "    \n",
    "    I tried to plot the predictions with earlier prices to see if the\n",
    "    model is valid. I expected a linear trend in the scatter\n",
    "    plot. What I could see was the trend was linear for the most part\n",
    "    with more data points scattered when the model complexity was 4\n",
    "    than when it was 7. I got predictions in the range of 19.0 to\n",
    "    21.0.\n",
    "    \n",
    "    Example scatter plot from a run:\n",
    "    \n",
    "![alt text](housing_predictions.png \"Housing Predictions vs Actuals\")\n",
    "\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
